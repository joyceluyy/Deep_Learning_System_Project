{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "christian-cookbook",
   "metadata": {},
   "source": [
    "### Dialogue Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rubber-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import cPickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-action",
   "metadata": {},
   "source": [
    "#### Step 1: Load the vocabulary from the data/vocab.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lucky-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join('data', \"vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minor-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary \n",
    "with open(vocab_file, \"rb\") as f:\n",
    "    word_counts, word_to_index, index_to_word = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-platform",
   "metadata": {},
   "source": [
    "#### Step 2: Build the LSTM model with pre-trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "short-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    rnn_size = 256 # size of RNN\n",
    "    learning_rate = 0.001 #learning rate\n",
    "    \n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "\n",
    "def load_model(weight_file):\n",
    "    # Load the network weights \n",
    "    model = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "    model.load_weights(weight_file)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-bulgarian",
   "metadata": {},
   "source": [
    "#### Step 3: Provide the seed sententense and start text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divided-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chara(text):\n",
    "    SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "    token_dict = {\n",
    "            '.': '||Period||',\n",
    "            ',': '||Comma||',\n",
    "            '\"': '||Quotation_Mark||',\n",
    "            ';': '||Semicolon||',\n",
    "            '!': '||Exclamation_Mark||',\n",
    "            '?': '||Question_Mark||',\n",
    "            '(': '||Left_Parentheses||',\n",
    "            ')': '||Right_Parentheses||',\n",
    "            '-': '||Dash||',\n",
    "            '\\n': '||Return||'\n",
    "            }\n",
    "    for key, token in token_dict.items():\n",
    "            text = text.replace(\" {}\".format(token), key)\n",
    "    \n",
    "    text = text.replace('<PAD>', '\\n')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "growing-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing special characters in the text and pend <PAD> by the end of each conversation\n",
    "# Parse the string to list of tokens \n",
    "def tokenizer(text):\n",
    "    SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "    token_dict = {\n",
    "            '.': '||Period||',\n",
    "            ',': '||Comma||',\n",
    "            '\"': '||Quotation_Mark||',\n",
    "            ';': '||Semicolon||',\n",
    "            '!': '||Exclamation_Mark||',\n",
    "            '?': '||Question_Mark||',\n",
    "            '(': '||Left_Parentheses||',\n",
    "            ')': '||Right_Parentheses||',\n",
    "            '-': '||Dash||',\n",
    "            '\\n': '||Return||'\n",
    "            }\n",
    "    for key, token in token_dict.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "            \n",
    "    text = text.split()\n",
    "    text = text + list(SPECIAL_WORDS.values())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "joined-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    file = open(data_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    training_data = []\n",
    "    \n",
    "    for line in lines:\n",
    "        training_data.extend(tokenizer(line))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "def prepare_training_data(data, seq_length=20): \n",
    "    sequences_step = 1\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for i in range(0, len(data) - seq_length, sequences_step):\n",
    "        X_train.append(data[i: i + seq_length])\n",
    "        Y_train.append(data[i + seq_length])\n",
    "\n",
    "    print('Total sequences:', len(X_train))\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hairy-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "def dialogue_prediction(model, seed=None, prediction_length=100, data_path=None):\n",
    "    if seed == None:  \n",
    "        # Pick a random seed from the training set \n",
    "        X_train, Y_train = prepare_training_data(load_data(data_path))\n",
    "        start = np.random.randint(1, len(X_train)-1)\n",
    "        seed = X_train[start]\n",
    "    else:\n",
    "        seed = tokenizer(seed)[:20]\n",
    "        \n",
    "    print(\"Seed:\")\n",
    "    print(remove_special_chara(\" \" + \" \".join(seed)))\n",
    "\n",
    "    prediction = []\n",
    "    for i in range(prediction_length):\n",
    "        x = np.zeros((1, seq_length, vocab_size), dtype=np.bool)        \n",
    "        for j, word in enumerate(seed):\n",
    "            x[0, j, word_to_index[word]] = 1\n",
    "\n",
    "        y_hat = model.predict(x)\n",
    "        index = np.argmax(y_hat)\n",
    "        result = index_to_word[index]\n",
    "\n",
    "        prediction.append(result)\n",
    "        seed.append(result)\n",
    "        seed = seed[1:len(seed)]   \n",
    "\n",
    "    print(\"\\n Predicted diaglogue: \\n\")\n",
    "    print(remove_special_chara(\" \" + \" \".join(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-agenda",
   "metadata": {},
   "source": [
    " - Load the model for specific characters and predict the character speech based on seed sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "different-bacteria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Seed:\n",
      " jerry: well, senator, id just like to know, what you knew and when you knew it.\n",
      "\n",
      " Predicted diaglogue: \n",
      "\n",
      "\n",
      " \n",
      " jerry: can you relax, its a cup of coffee. claire is a professional waitress.\n",
      " \n",
      " jerry: well, theres this uh, woman might be comin in.\n",
      " \n",
      " jerry: i told you about laura, the girl i met in michigan?\n",
      " \n",
      " jerry: i thought you said we had a little thing about it was the street.\n",
      " \n",
      " jerry: but it's not to keep a little more than a good time.\n",
      " \n",
      " jerry: well, it was great. no we're on the phone.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"save/model_lstm_jerry_128.40-0.41.hdf5\")\n",
    "dialogue_prediction(model, \"jerry: well, senator, id just like to know, what you knew and when you knew it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subject-beach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Seed:\n",
      " elaine: i think, i think we were in my house where i grew up, and you were standing\n",
      "\n",
      " Predicted diaglogue: \n",
      "\n",
      " there, you were getting involved in the same.\n",
      " \n",
      " elaine: uh, no. no. i was just in the invitation, or a hundred and went in a good.\n",
      " \n",
      " elaine: oh, my god.\n",
      " \n",
      " elaine: oh, lighten up. we're going home. i'm going to hell.\n",
      " \n",
      " elaine: come on, come on, okay.\n",
      " \n",
      " elaine: ill get out of the seat.\n",
      " \n",
      " elaine: hey!\n",
      " \n",
      " elaine: oh, god.\n",
      " \n",
      " elaine: no, i won't\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"save/model_lstm_elaine_128.28-0.95.hdf5\")\n",
    "dialogue_prediction(model, \"elaine: i think, i think we were in my house where i grew up, and you were standing there, you were looking out the window...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "closing-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total sequences: 32339\n",
      "Seed:\n",
      " a second!( they fight over the paper.)\n",
      " \n",
      " jerry: all right, here!\n",
      "\n",
      "\n",
      " Predicted diaglogue: \n",
      "\n",
      " \n",
      " jerry:( reading the guard, reading) hey!\n",
      " \n",
      " jerry: look at this shirt!\n",
      " \n",
      " jerry: i got a little! how could you explain this?\n",
      " \n",
      " jerry: so, why don't you just just just just just wanted to pick you up?\n",
      " \n",
      " jerry: you know, maybe if maybe we could start a little more.\n",
      " \n",
      " jerry: it is not the most more lane and not working.\n",
      " \n",
      " jerry: for it?\n",
      " \n",
      " jerry: is it that a lot of coffee in the\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"save/model_lstm_jerry_128.40-0.41.hdf5\")\n",
    "dialogue_prediction(model, seed=None, prediction_length=100, data_path=\"data/jerry_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-vietnamese",
   "metadata": {},
   "source": [
    "- Load the multi-character dialogue \n",
    "- Predict the dialogue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "detailed-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total sequences: 645871\n",
      "Seed:\n",
      " their buying bone.\n",
      " \n",
      " jerry: hey, you know what? this is all your mail. they're\n",
      "\n",
      " Predicted diaglogue: \n",
      "\n",
      " puttin' it in my face.\n",
      " \n",
      " jerry: i don't know what you do.\n",
      " \n",
      " george: i can't believe this guy.\n",
      " \n",
      " jerry: i can't believe this guy. i can't believe he hasn't called me to see.\n",
      " \n",
      " elaine: what are you doing?\n",
      " \n",
      " jerry: i don't know.\n",
      " \n",
      " elaine: what?\n",
      " \n",
      " jerry: i don't know.\n",
      " \n",
      " elaine: you know, i think this is a little awkward.\n",
      " \n",
      " elaine: what is this?\n",
      " \n",
      " jerry: i think i can do\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"save/model_lstm_dialogue_128.14-1.87.hdf5\")\n",
    "dialogue_prediction(model, seed=None, prediction_length=100, data_path=\"data/main_character_script.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-fifth",
   "metadata": {},
   "source": [
    "### Evaluate the prediction dialogue "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-occupation",
   "metadata": {},
   "source": [
    "Load the Training data, we will random select the seed sentense and compute the BLEU and Rouge score of the text generation. Take Jerry's line as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "experienced-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prediction sentence together with the reference \n",
    "\n",
    "def reference_prediction_gen(model, X_train, Y_train, prediction_length=100):\n",
    "    # Pick a random seed from the training set \n",
    "    start = np.random.randint(1, len(X_train)-1)\n",
    "    seed = X_train[start]\n",
    "\n",
    "    prediction = []\n",
    "    for i in range(prediction_length):\n",
    "        x = np.zeros((1, seq_length, vocab_size), dtype=np.bool)        \n",
    "        for j, word in enumerate(seed):\n",
    "            x[0, j, word_to_index[word]] = 1\n",
    "\n",
    "        y_hat = model.predict(x)\n",
    "        index = np.argmax(y_hat)\n",
    "        result = index_to_word[index]\n",
    "\n",
    "        prediction.append(result)\n",
    "        seed.append(result)\n",
    "        seed = seed[1:len(seed)]   \n",
    "\n",
    "    reference = Y_train[start:start+prediction_length]\n",
    "    \n",
    "    return reference, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "compound-retail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"save/model_lstm_jerry_128.40-0.41.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "meaningful-venezuela",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 32339\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = prepare_training_data(load_data(\"data/jerry_test.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-resolution",
   "metadata": {},
   "source": [
    "### BLEU Score \n",
    "- BLEU is a precision focused metrics calculating n-gram overlap between the reference and prediction text. \n",
    "- We evaluate the BLEU-1 and BLEU-4 scores for multiple simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "biblical-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def BLEU_score(simulation_time, prediction_length):\n",
    "    avg_BLEU_1 = 0\n",
    "    avg_BLEU_4 = 0\n",
    "    \n",
    "    for i in range(simulation_time):\n",
    "        reference, prediction = reference_prediction_gen(model, X_train, Y_train, prediction_length)\n",
    "        BLEU_1 = nltk.translate.bleu_score.sentence_bleu([reference], prediction, weights=(1,0,0,0))\n",
    "        BLEU_4 = nltk.translate.bleu_score.sentence_bleu([reference], prediction)\n",
    "        \n",
    "        avg_BLEU_1 += BLEU_1\n",
    "        avg_BLEU_4 += BLEU_4\n",
    "        \n",
    "    avg_BLEU_1 = avg_BLEU_1 / simulation_time\n",
    "    avg_BLEU_4 = avg_BLEU_4 / simulation_time\n",
    "    \n",
    "    print(f\"Cumulative 1-gram: {avg_BLEU_1}\")\n",
    "    print(f\"Cumulative 4-gram: {avg_BLEU_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "injured-sudan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joycelu/Library/Python/3.8/lib/python/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.45539999999999997\n",
      "Cumulative 4-gram: 0.1934442068434737\n"
     ]
    }
   ],
   "source": [
    "BLEU_score(simulation_time = 100, prediction_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-notification",
   "metadata": {},
   "source": [
    "### Rouge Score\n",
    "- Versus the precision forcused BLEU score, Rouge score focues more on recall. \n",
    "- Rouge-N: overlap of N-gram (we tested for unigram and bigram)\n",
    "- Rouge-L: Longest common sequence (LCS)\n",
    "- Rouge-W: Weighted LCS-based statistics that favors consecutive LCSes\n",
    "- Rouge-S4: Skip-gram concurrence with skip gap 4\n",
    "- Rouge-SU4: Skip-gram plus unigram concurrence with skip gap 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "empirical-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_metric import PyRouge\n",
    "\n",
    "def Rouge_score(simulation_time, prediction_length):\n",
    "    \n",
    "    for i in range(simulation_time):\n",
    "        reference, prediction = reference_prediction_gen(model, X_train, Y_train, prediction_length)\n",
    "        reference = remove_special_chara(\" \" + \" \".join(reference))\n",
    "        prediction = remove_special_chara(\" \" + \" \".join(prediction))\n",
    "\n",
    "        rouge = PyRouge(rouge_n=(1, 2), rouge_l=True, rouge_w=True,\n",
    "                    rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
    "        scores = rouge.evaluate([prediction], [[reference]])\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intended-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.3284671532846715, 'p': 0.3125, 'f': 0.3202846975088968}, 'rouge-2': {'r': 0.022058823529411766, 'p': 0.02097902097902098, 'f': 0.021505376344086023}, 'rouge-l': {'r': 0.31386861313868614, 'p': 0.2986111111111111, 'f': 0.30604982206405695}, 'rouge-w-1.2': {'r': 0.10404684205378563, 'p': 0.17210700842124219, 'f': 0.1296899586281442}, 'rouge-s4': {'r': 0.041791044776119404, 'p': 0.03971631205673759, 'f': 0.04072727272727273}, 'rouge-su4': {'r': 0.08933002481389578, 'p': 0.08490566037735849, 'f': 0.08706166868198306}}\n"
     ]
    }
   ],
   "source": [
    "Rouge_score(1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-temperature",
   "metadata": {},
   "source": [
    "### Perplexity \n",
    "- we caluclate the unigram perplexity \n",
    "\n",
    "$p(w) = \\frac{count(w)}{count(vcab)}$\n",
    "\n",
    "$PP(s) = 2^{log_{2}^{PP(s)}} = 2^{-\\frac{1}{n}log(p(s))}$\n",
    "\n",
    "$\\frac{1}{n}log(p(s)) = \\frac{1}{n}(logp(w_{1})+logp(w_{2})+...+logp(w_{n}))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "common-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the unigram probability in the vocabulary \n",
    "# word_counts load from the vocab.pkl file contains the word and its count in vocab \n",
    "\n",
    "unigram_prob = {}\n",
    "for w in word_counts:\n",
    "    unigram_prob[w[0]] = w[1]\n",
    "\n",
    "def perplexity_score(simulation_time, prediction_length):\n",
    "    score = 0.0\n",
    "    for i in range(simulation_time):\n",
    "        reference, prediction = reference_prediction_gen(model, X_train, Y_train, prediction_length)\n",
    "        l = 0.0\n",
    "        for w in prediction:\n",
    "            l += np.log2(unigram_prob[w])\n",
    "        l = l / prediction_length\n",
    "        score += np.power(2, -l)\n",
    "    score /= simulation_time\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "invisible-corporation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00019151019468558275"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_score(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-smell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
