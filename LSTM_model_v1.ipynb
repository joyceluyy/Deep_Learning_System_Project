{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import codecs\n",
    "import collections\n",
    "import argparse\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chronic-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'# data directory containing scripts.csv\n",
    "save_dir = 'save' # directory to store checkpointed models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-reading",
   "metadata": {},
   "source": [
    "### Load the script data\n",
    "Parse all the scripts to a dictionary based on the character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(data_dir, \"scripts.csv\")\n",
    "vocab_file = os.path.join(data_dir, \"vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "labeled-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the csv file to a dictionary with character as the key \n",
    "dialogue_dict = {}\n",
    "\n",
    "input_data = csv.DictReader(open(input_file))\n",
    "for row in input_data:\n",
    "    dialogue = row[\"Character\"].lower() + \": \" + row[\"Dialogue\"].lower() + \"\\n\"\n",
    "    if row[\"Character\"].lower() in dialogue_dict:\n",
    "        dialogue_dict[row[\"Character\"].lower()].append(dialogue)\n",
    "    else:\n",
    "        dialogue_dict[row[\"Character\"].lower()] = [dialogue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "studied-remark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jerry', 14786),\n",
       " ('george', 9708),\n",
       " ('elaine', 7984),\n",
       " ('kramer', 6664),\n",
       " ('newman', 641),\n",
       " ('morty', 505)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List the first 6 characters with the most lines\n",
    "dialogue_freq = {}\n",
    "\n",
    "for key in dialogue_dict:\n",
    "    dialogue_freq[key] = len(dialogue_dict[key])\n",
    "    \n",
    "highest_freq = collections.Counter(dialogue_freq).most_common(6)\n",
    "highest_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-found",
   "metadata": {},
   "source": [
    "There are 4 major characters in the TV series, other characters has much less lines compared to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-theater",
   "metadata": {},
   "source": [
    "### Build vocabulary \n",
    "Build the vocabulary, word_to_index mapping, based on all the words appeared in the scripts.csv file. And save the vocabulary to vocab.pkl file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unauthorized-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocalbuary is built by all the possible characters in the scripts.csv\n",
    "\n",
    "# Replacing special characters in the text and pend <PAD> by the end of each conversation\n",
    "# Parse the string to list of tokens \n",
    "def tokenizer(text):\n",
    "    SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "    token_dict = {\n",
    "            '.': '||Period||',\n",
    "            ',': '||Comma||',\n",
    "            '\"': '||Quotation_Mark||',\n",
    "            ';': '||Semicolon||',\n",
    "            '!': '||Exclamation_Mark||',\n",
    "            '?': '||Question_Mark||',\n",
    "            '(': '||Left_Parentheses||',\n",
    "            ')': '||Right_Parentheses||',\n",
    "            '-': '||Dash||',\n",
    "            '\\n': '||Return||'\n",
    "            }\n",
    "    for key, token in token_dict.items():\n",
    "            text = text.replace(key, ' {} '.format(token))\n",
    "            \n",
    "    text = text.split()\n",
    "    text = text + list(SPECIAL_WORDS.values())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arctic-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "\n",
    "for character, dialogue_lst in dialogue_dict.items():\n",
    "    for dialogue in dialogue_lst:\n",
    "        vocabulary.extend(tokenizer(dialogue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amazing-rebound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 21397\n"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(vocabulary).most_common()\n",
    "\n",
    "# Find the unique word lists \n",
    "words_lst = [x[0] for x in word_counts]\n",
    "words_lst = list(sorted(words_lst))\n",
    "\n",
    "# Mapping from word to index\n",
    "word_to_index = {x: i for i, x in enumerate(words_lst)}\n",
    "index_to_word = {i: x for i, x in enumerate(words_lst)}\n",
    "words = [x[0] for x in word_counts]\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "#Save the vovabulary file\n",
    "with open(vocab_file, 'wb') as f:\n",
    "    cPickle.dump((word_counts, word_to_index, index_to_word), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-necklace",
   "metadata": {},
   "source": [
    "### Prepare Training Data for individual character\n",
    "Create training data (x and y) for each character based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "constant-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dialogue data for specific character \n",
    "def get_script(character):\n",
    "    path = os.path.join(\"data\", character + \"_script.txt\")\n",
    "    file = open(path, \"w+\")\n",
    "    for line in dialogue_dict[character]:\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brief-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the script for the major four characters \n",
    "main_characters = ['jerry', 'george', 'elaine', 'kramer']\n",
    "for c in main_characters:\n",
    "    get_script(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acceptable-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    file = open(data_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    training_data = []\n",
    "    \n",
    "    for line in lines:\n",
    "        training_data.extend(tokenizer(line))\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "boxed-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data, seq_length): \n",
    "    sequences_step = 1\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for i in range(0, len(data) - seq_length, sequences_step):\n",
    "        X_train.append(data[i: i + seq_length])\n",
    "        Y_train.append(data[i + seq_length])\n",
    "\n",
    "    print('Total sequences:', len(X_train))\n",
    "    return X_train, Y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supported-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_train, Y_train, epochs, batch_size):\n",
    "    for _ in range(epochs):\n",
    "        cursor = 0\n",
    "        while cursor + batch_size < len(X_train):\n",
    "            x_batch = X_train[cursor:cursor+batch_size]\n",
    "            y_batch = Y_train[cursor:cursor+batch_size]\n",
    "            \n",
    "            x = np.zeros((len(x_batch), seq_length, vocab_size), dtype=np.bool)\n",
    "            y = np.zeros((len(y_batch), vocab_size), dtype=np.bool)\n",
    "            for i, sentence in enumerate(x_batch):\n",
    "                for j, word in enumerate(sentence):\n",
    "                    x[i, j, word_to_index[word]] = 1\n",
    "                y[i, word_to_index[y_batch[i]]] = 1\n",
    "            yield x, y\n",
    "            cursor += batch_size\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-stomach",
   "metadata": {},
   "source": [
    "### Prepare Training data for inter character dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incident-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building training data sets for four major characters in the sequence of the dialogue \n",
    "main_characters = ['jerry', 'george', 'elaine', 'kramer']\n",
    "\n",
    "def get_dialogue():\n",
    "    input_data = csv.DictReader(open(input_file))\n",
    "    path = os.path.join(\"data\", \"main_character_script.txt\")\n",
    "    file = open(path, \"w+\")\n",
    "    for row in input_data:\n",
    "        if row[\"Character\"].lower() in main_characters:\n",
    "            line = row[\"Character\"].lower() + \": \" + row[\"Dialogue\"].lower() + \"\\n\"\n",
    "            file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dialogue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-colors",
   "metadata": {},
   "source": [
    "### Define the LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "golden-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    rnn_size = 256 # size of RNN\n",
    "    learning_rate = 0.001 #learning rate\n",
    "    \n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-carbon",
   "metadata": {},
   "source": [
    "### Training Per character dataset\n",
    "Train the model specifically for each main charaters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "isolated-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "batch_size = 128 # minibatch size\n",
    "num_epochs = 40 # number of epochs\n",
    "seq_length = 20\n",
    "\n",
    "def model_training(data_path):\n",
    "    model = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "    model.summary()\n",
    "    \n",
    "    X_train, Y_train = prepare_training_data(load_data(data_path), seq_length)\n",
    "\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='loss'),\n",
    "               ModelCheckpoint(filepath=save_dir + \"/\" + f'model_lstm_{character}_{batch_size}' + '.{epoch:02d}-{loss:.2f}.hdf5',\n",
    "                               monitor='loss', verbose=0, mode='auto', period=2)]\n",
    "    #fit the model\n",
    "    str_time = time.time()\n",
    "    history = model.fit(batch_generator(X_train, Y_train, num_epochs, batch_size),\n",
    "                     batch_size=batch_size,\n",
    "                     steps_per_epoch = np.floor(len(X_train)/batch_size),\n",
    "                     epochs=num_epochs,\n",
    "                     callbacks=callbacks)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - str_time\n",
    "    \n",
    "    #save the model history \n",
    "    with open(save_dir + \"/\" + f'model_history_{character}_{batch_size}.pkl', 'wb') as file_pi:\n",
    "        cPickle.dump((history.history, training_time), file_pi)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "driven-observation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with george script\n",
      "Build LSTM model.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total sequences: 170682\n",
      "Epoch 1/40\n",
      "1333/1333 [==============================] - 175s 129ms/step - loss: 6.2189 - categorical_accuracy: 0.1213\n",
      "Epoch 2/40\n",
      "1333/1333 [==============================] - 175s 131ms/step - loss: 4.6531 - categorical_accuracy: 0.2554\n",
      "Epoch 3/40\n",
      "1333/1333 [==============================] - 169s 127ms/step - loss: 4.2874 - categorical_accuracy: 0.2842\n",
      "Epoch 4/40\n",
      "1333/1333 [==============================] - 166s 124ms/step - loss: 3.9990 - categorical_accuracy: 0.3094\n",
      "Epoch 5/40\n",
      "1333/1333 [==============================] - 167s 125ms/step - loss: 3.7583 - categorical_accuracy: 0.3270\n",
      "Epoch 6/40\n",
      "1333/1333 [==============================] - 166s 124ms/step - loss: 3.5358 - categorical_accuracy: 0.3412\n",
      "Epoch 7/40\n",
      "1333/1333 [==============================] - 168s 126ms/step - loss: 3.3384 - categorical_accuracy: 0.3524\n",
      "Epoch 8/40\n",
      "1333/1333 [==============================] - 169s 127ms/step - loss: 3.1310 - categorical_accuracy: 0.3684\n",
      "Epoch 9/40\n",
      "1333/1333 [==============================] - 169s 126ms/step - loss: 2.9434 - categorical_accuracy: 0.3839\n",
      "Epoch 10/40\n",
      "1333/1333 [==============================] - 165s 124ms/step - loss: 2.7433 - categorical_accuracy: 0.4056\n",
      "Epoch 11/40\n",
      "1333/1333 [==============================] - 165s 124ms/step - loss: 2.5639 - categorical_accuracy: 0.4309\n",
      "Epoch 12/40\n",
      "1333/1333 [==============================] - 165s 124ms/step - loss: 2.3932 - categorical_accuracy: 0.4585\n",
      "Epoch 13/40\n",
      "1333/1333 [==============================] - 165s 124ms/step - loss: 2.2371 - categorical_accuracy: 0.4858\n",
      "Epoch 14/40\n",
      "1333/1333 [==============================] - 166s 125ms/step - loss: 2.0901 - categorical_accuracy: 0.5119\n",
      "Epoch 15/40\n",
      "1333/1333 [==============================] - 164s 123ms/step - loss: 1.9441 - categorical_accuracy: 0.5390\n",
      "Epoch 16/40\n",
      "1333/1333 [==============================] - 163s 122ms/step - loss: 1.8312 - categorical_accuracy: 0.5618\n",
      "Epoch 17/40\n",
      "1333/1333 [==============================] - 162s 122ms/step - loss: 1.7205 - categorical_accuracy: 0.5825\n",
      "Epoch 18/40\n",
      "1333/1333 [==============================] - 163s 122ms/step - loss: 1.6096 - categorical_accuracy: 0.6039\n",
      "Epoch 19/40\n",
      "1333/1333 [==============================] - 163s 122ms/step - loss: 1.5196 - categorical_accuracy: 0.6230\n",
      "Epoch 20/40\n",
      "1333/1333 [==============================] - 163s 122ms/step - loss: 1.4240 - categorical_accuracy: 0.6439\n",
      "Epoch 21/40\n",
      "1333/1333 [==============================] - 162s 122ms/step - loss: 1.3488 - categorical_accuracy: 0.6599\n",
      "Epoch 22/40\n",
      "1333/1333 [==============================] - 162s 122ms/step - loss: 1.2666 - categorical_accuracy: 0.6769\n",
      "Epoch 23/40\n",
      "1333/1333 [==============================] - 162s 122ms/step - loss: 1.2085 - categorical_accuracy: 0.6907\n",
      "Epoch 24/40\n",
      "1333/1333 [==============================] - 162s 122ms/step - loss: 1.1400 - categorical_accuracy: 0.7067\n",
      "Epoch 25/40\n",
      "1333/1333 [==============================] - 163s 122ms/step - loss: 1.0662 - categorical_accuracy: 0.7251\n",
      "Epoch 26/40\n",
      "1333/1333 [==============================] - 177s 133ms/step - loss: 1.0082 - categorical_accuracy: 0.7390\n",
      "Epoch 27/40\n",
      "1333/1333 [==============================] - 180s 135ms/step - loss: 0.9381 - categorical_accuracy: 0.7548\n",
      "Epoch 28/40\n",
      "1333/1333 [==============================] - 179s 134ms/step - loss: 0.8847 - categorical_accuracy: 0.7680\n",
      "Epoch 29/40\n",
      "1333/1333 [==============================] - 179s 134ms/step - loss: 0.8452 - categorical_accuracy: 0.7765\n",
      "Epoch 30/40\n",
      "1333/1333 [==============================] - 179s 135ms/step - loss: 0.7966 - categorical_accuracy: 0.7873\n",
      "Epoch 31/40\n",
      "1333/1333 [==============================] - 179s 134ms/step - loss: 0.7374 - categorical_accuracy: 0.8040\n",
      "Epoch 32/40\n",
      "1333/1333 [==============================] - 179s 134ms/step - loss: 0.6971 - categorical_accuracy: 0.8130\n",
      "Epoch 33/40\n",
      "1333/1333 [==============================] - 179s 134ms/step - loss: 0.6560 - categorical_accuracy: 0.8233\n",
      "Epoch 34/40\n",
      "1333/1333 [==============================] - 178s 133ms/step - loss: 0.6311 - categorical_accuracy: 0.8295\n",
      "Epoch 35/40\n",
      "1333/1333 [==============================] - 178s 133ms/step - loss: 0.6100 - categorical_accuracy: 0.8341\n",
      "Epoch 36/40\n",
      "1333/1333 [==============================] - 178s 133ms/step - loss: 0.5744 - categorical_accuracy: 0.8436\n",
      "Epoch 37/40\n",
      "1333/1333 [==============================] - 177s 133ms/step - loss: 0.5422 - categorical_accuracy: 0.8526\n",
      "Epoch 38/40\n",
      "1333/1333 [==============================] - 178s 133ms/step - loss: 0.5400 - categorical_accuracy: 0.8526\n",
      "Epoch 39/40\n",
      "1333/1333 [==============================] - 179s 135ms/step - loss: 0.5797 - categorical_accuracy: 0.8410\n",
      "Model Training with elaine script\n",
      "Build LSTM model.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total sequences: 124599\n",
      "Epoch 1/40\n",
      "973/973 [==============================] - 133s 135ms/step - loss: 5.5569 - categorical_accuracy: 0.1717\n",
      "Epoch 2/40\n",
      "973/973 [==============================] - 130s 133ms/step - loss: 4.4249 - categorical_accuracy: 0.2780\n",
      "Epoch 3/40\n",
      "973/973 [==============================] - 132s 136ms/step - loss: 4.1651 - categorical_accuracy: 0.3025\n",
      "Epoch 4/40\n",
      "973/973 [==============================] - 129s 133ms/step - loss: 3.9363 - categorical_accuracy: 0.3231\n",
      "Epoch 5/40\n",
      "973/973 [==============================] - 129s 133ms/step - loss: 3.7134 - categorical_accuracy: 0.3423\n",
      "Epoch 6/40\n",
      "973/973 [==============================] - 129s 133ms/step - loss: 3.5179 - categorical_accuracy: 0.3576\n",
      "Epoch 7/40\n",
      "973/973 [==============================] - 129s 132ms/step - loss: 3.3277 - categorical_accuracy: 0.3714\n",
      "Epoch 8/40\n",
      "973/973 [==============================] - 128s 132ms/step - loss: 3.1523 - categorical_accuracy: 0.3838\n",
      "Epoch 9/40\n",
      "973/973 [==============================] - 128s 132ms/step - loss: 2.9878 - categorical_accuracy: 0.3958\n",
      "Epoch 10/40\n",
      "973/973 [==============================] - 130s 134ms/step - loss: 2.8209 - categorical_accuracy: 0.4094\n",
      "Epoch 11/40\n",
      "973/973 [==============================] - 131s 134ms/step - loss: 2.6628 - categorical_accuracy: 0.4257\n",
      "Epoch 12/40\n",
      "973/973 [==============================] - 128s 131ms/step - loss: 2.5285 - categorical_accuracy: 0.4430\n",
      "Epoch 13/40\n",
      "973/973 [==============================] - 126s 130ms/step - loss: 2.4045 - categorical_accuracy: 0.4622\n",
      "Epoch 14/40\n",
      "973/973 [==============================] - 127s 130ms/step - loss: 2.2315 - categorical_accuracy: 0.4888\n",
      "Epoch 15/40\n",
      "973/973 [==============================] - 128s 132ms/step - loss: 2.5629 - categorical_accuracy: 0.4770\n",
      "Epoch 16/40\n",
      "973/973 [==============================] - 124s 127ms/step - loss: 2.1376 - categorical_accuracy: 0.5073\n",
      "Epoch 17/40\n",
      "973/973 [==============================] - 124s 127ms/step - loss: 1.9642 - categorical_accuracy: 0.5408\n",
      "Epoch 18/40\n",
      "973/973 [==============================] - 127s 131ms/step - loss: 1.7910 - categorical_accuracy: 0.5705\n",
      "Epoch 19/40\n",
      "973/973 [==============================] - 130s 134ms/step - loss: 1.6713 - categorical_accuracy: 0.5912\n",
      "Epoch 20/40\n",
      "973/973 [==============================] - 130s 133ms/step - loss: 1.5693 - categorical_accuracy: 0.6138\n",
      "Epoch 21/40\n",
      "973/973 [==============================] - 130s 134ms/step - loss: 1.4597 - categorical_accuracy: 0.6356\n",
      "Epoch 22/40\n",
      "973/973 [==============================] - 129s 132ms/step - loss: 1.3793 - categorical_accuracy: 0.6526\n",
      "Epoch 23/40\n",
      "973/973 [==============================] - 129s 132ms/step - loss: 1.3150 - categorical_accuracy: 0.6677\n",
      "Epoch 24/40\n",
      "973/973 [==============================] - 128s 132ms/step - loss: 1.2322 - categorical_accuracy: 0.6842\n",
      "Epoch 25/40\n",
      "973/973 [==============================] - 128s 131ms/step - loss: 1.1668 - categorical_accuracy: 0.7011\n",
      "Epoch 26/40\n",
      "973/973 [==============================] - 128s 131ms/step - loss: 1.0947 - categorical_accuracy: 0.7193\n",
      "Epoch 27/40\n",
      "973/973 [==============================] - 128s 131ms/step - loss: 1.0169 - categorical_accuracy: 0.7339\n",
      "Epoch 28/40\n",
      "973/973 [==============================] - 127s 131ms/step - loss: 0.9547 - categorical_accuracy: 0.7498\n",
      "Epoch 29/40\n",
      "973/973 [==============================] - 127s 131ms/step - loss: 1.2774 - categorical_accuracy: 0.7227\n",
      "Epoch 30/40\n",
      "973/973 [==============================] - 126s 130ms/step - loss: 1.0775 - categorical_accuracy: 0.7227\n",
      "Model Training with kramer script\n",
      "Build LSTM model.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 512)               44347392  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21397)             10976661  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 21397)             0         \n",
      "=================================================================\n",
      "Total params: 55,324,053\n",
      "Trainable params: 55,324,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total sequences: 114373\n",
      "Epoch 1/40\n",
      "893/893 [==============================] - 119s 131ms/step - loss: 7.3166 - categorical_accuracy: 0.0631\n",
      "Epoch 2/40\n",
      "893/893 [==============================] - 118s 132ms/step - loss: 4.7469 - categorical_accuracy: 0.2389\n",
      "Epoch 3/40\n",
      "893/893 [==============================] - 120s 134ms/step - loss: 4.2922 - categorical_accuracy: 0.2889\n",
      "Epoch 4/40\n",
      "893/893 [==============================] - 120s 134ms/step - loss: 3.9452 - categorical_accuracy: 0.3226\n",
      "Epoch 5/40\n",
      "893/893 [==============================] - 119s 133ms/step - loss: 3.6579 - categorical_accuracy: 0.3424\n",
      "Epoch 6/40\n",
      "893/893 [==============================] - 118s 132ms/step - loss: 3.4022 - categorical_accuracy: 0.3580\n",
      "Epoch 7/40\n",
      "893/893 [==============================] - 117s 131ms/step - loss: 3.1902 - categorical_accuracy: 0.3715\n",
      "Epoch 8/40\n",
      "893/893 [==============================] - 117s 131ms/step - loss: 2.9876 - categorical_accuracy: 0.3872\n",
      "Epoch 9/40\n",
      "893/893 [==============================] - 118s 132ms/step - loss: 2.7956 - categorical_accuracy: 0.4031\n",
      "Epoch 10/40\n",
      "893/893 [==============================] - 117s 131ms/step - loss: 2.5923 - categorical_accuracy: 0.4295\n",
      "Epoch 11/40\n",
      "893/893 [==============================] - 117s 130ms/step - loss: 2.4182 - categorical_accuracy: 0.4547\n",
      "Epoch 12/40\n",
      "893/893 [==============================] - 117s 131ms/step - loss: 2.2396 - categorical_accuracy: 0.4837\n",
      "Epoch 13/40\n",
      "893/893 [==============================] - 116s 129ms/step - loss: 2.0907 - categorical_accuracy: 0.5108\n",
      "Epoch 14/40\n",
      "893/893 [==============================] - 115s 129ms/step - loss: 1.9487 - categorical_accuracy: 0.5363\n",
      "Epoch 15/40\n",
      "893/893 [==============================] - 116s 130ms/step - loss: 1.8824 - categorical_accuracy: 0.5567\n",
      "Epoch 16/40\n",
      "893/893 [==============================] - 115s 128ms/step - loss: 1.6856 - categorical_accuracy: 0.5870\n",
      "Epoch 17/40\n",
      "893/893 [==============================] - 114s 128ms/step - loss: 1.5590 - categorical_accuracy: 0.6129\n",
      "Epoch 18/40\n",
      "893/893 [==============================] - 112s 126ms/step - loss: 1.4333 - categorical_accuracy: 0.6401\n",
      "Epoch 19/40\n",
      "893/893 [==============================] - 113s 127ms/step - loss: 1.3339 - categorical_accuracy: 0.6622\n",
      "Epoch 20/40\n",
      "893/893 [==============================] - 113s 127ms/step - loss: 1.2487 - categorical_accuracy: 0.6797\n",
      "Epoch 21/40\n",
      "893/893 [==============================] - 113s 127ms/step - loss: 1.1881 - categorical_accuracy: 0.6891\n",
      "Epoch 22/40\n",
      "893/893 [==============================] - 113s 126ms/step - loss: 1.0856 - categorical_accuracy: 0.7163\n",
      "Epoch 23/40\n",
      "893/893 [==============================] - 111s 124ms/step - loss: 1.0153 - categorical_accuracy: 0.7317\n",
      "Epoch 24/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.9406 - categorical_accuracy: 0.7507\n",
      "Epoch 25/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.8801 - categorical_accuracy: 0.7659\n",
      "Epoch 26/40\n",
      "893/893 [==============================] - 109s 123ms/step - loss: 0.8151 - categorical_accuracy: 0.7797\n",
      "Epoch 27/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.7428 - categorical_accuracy: 0.7990\n",
      "Epoch 28/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.6768 - categorical_accuracy: 0.8149\n",
      "Epoch 29/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.6273 - categorical_accuracy: 0.8270\n",
      "Epoch 30/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.5842 - categorical_accuracy: 0.8414\n",
      "Epoch 31/40\n",
      "893/893 [==============================] - 109s 123ms/step - loss: 0.5473 - categorical_accuracy: 0.8495\n",
      "Epoch 32/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.5039 - categorical_accuracy: 0.8605\n",
      "Epoch 33/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.4628 - categorical_accuracy: 0.8702\n",
      "Epoch 34/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.4244 - categorical_accuracy: 0.8804\n",
      "Epoch 35/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.4356 - categorical_accuracy: 0.8808\n",
      "Epoch 36/40\n",
      "893/893 [==============================] - 109s 122ms/step - loss: 0.3904 - categorical_accuracy: 0.8904\n",
      "Epoch 37/40\n",
      "893/893 [==============================] - 111s 125ms/step - loss: 0.3455 - categorical_accuracy: 0.9024\n",
      "Epoch 38/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.3515 - categorical_accuracy: 0.9021\n",
      "Epoch 39/40\n",
      "893/893 [==============================] - 110s 123ms/step - loss: 0.3756 - categorical_accuracy: 0.8955\n"
     ]
    }
   ],
   "source": [
    "main_characters = ['jerry', 'george', 'elaine', 'kramer']\n",
    "for c in main_characters:\n",
    "    print(f\"Model Training with {c} script\")\n",
    "    path = os.path.join(\"data\", character + \"_script.txt\")\n",
    "    model_training(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-discovery",
   "metadata": {},
   "source": [
    "### Train inter-character dialogue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data\", \"main_character_script.txt\")\n",
    "model_training(path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
